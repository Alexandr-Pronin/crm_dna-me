<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 428bcad8-bc44-4857-8626-12a70f011583 (2026-01-21 15:39Z) -->

# Etappe 4 event ingestion (2026-01-21 15:39Z)

_**User (2026-01-21 14:32Z)**_

Implement the following to-dos from the plan (the plan is attached for your reference). Do NOT edit the plan file itself.

You have been assigned the following 1 to-do(s) with IDs: etappe-4

1. [etappe-4] Etappe 4: Event Ingestion - Middleware, Queues, Events Route, Worker

These to-dos have already been created. Do not create them again. Mark them as in_progress as you work, starting with the first one. Don't stop until you have completed all the assigned to-dos.

---

_**Agent (model claude-4.5-opus-high-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
Der Benutzer möchte, dass ich Etappe 4 aus dem Plan implementiere: Event Ingestion. Das umfasst:

1. API-Key Middleware (4.1)
2. HMAC Middleware (4.2)
3. BullMQ Queues (4.3)
4. Event Validation Schema (4.4)
5. Events Route (4.5)
6. Registriere Routes (4.6)
7. Event Worker (4.7)
8. Workers Entry Point (4.8)
9. Teste Event Ingestion (4.9)

Zuerst muss ich die bestehenden Dateien und die Struktur des Projekts verstehen. Lassen Sie mich die relevanten Dateien lesen, um den aktuellen Stand zu verstehen.</details></think>

---

Ich werde Etappe 4: Event Ingestion implementieren. Zuerst markiere ich die Aufgabe als in Bearbeitung und schaue mir den aktuellen Stand des Projekts an.

---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [-] Etappe 4: Event Ingestion - Middleware, Queues, Events Route, Worker

</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** • Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** • Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Gut, es gibt bereits einige Basis-Dateien. Ich muss mir ansehen, was bereits existiert, um zu verstehen, was ich erstellen muss:

1. `src/config/index.ts` - Konfiguration
2. `src/config/redis.ts` - Redis-Konfiguration
3. `src/config/queues.ts` - BullMQ Queues (existiert schon!)
4. `src/db/index.ts` - Datenbankverbindung
5. `src/errors/index.ts` - Custom Error Classes
6. `src/types/index.ts` - TypeScript Types
7. `src/api/middleware/errorHandler.ts` - Error Handler
8. `src/index.ts` - Main Entry Point
9. `src/workers/index.ts` - Workers Entry Point

Ich muss die bestehenden Dateien lesen, um zu sehen, was ich noch erstellen muss.</details></think>

---

Gut, ich sehe die Projektstruktur. Lassen Sie mich die bestehenden Dateien lesen, um den aktuellen Stand zu verstehen.

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\config\index.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\config\redis.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\config\queues.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\types\index.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\errors\index.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\index.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\workers\index.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\db\index.ts</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Gut, ich habe einen guten Überblick über das bestehende Projekt. Jetzt muss ich die Dokumentation lesen, um die genauen Anforderungen zu verstehen.

Für Etappe 4: Event Ingestion brauche ich:

1. API-Key Middleware (4.1) - `src/api/middleware/apiKey.ts`
2. HMAC Middleware (4.2) - `src/api/middleware/hmac.ts`
3. BullMQ Queues (4.3) - BEREITS VORHANDEN in `src/config/queues.ts`
4. Event Validation Schema (4.4) - `src/api/schemas/events.ts`
5. Events Route (4.5) - `src/api/routes/events.ts`
6. Registriere Routes (4.6) - Update `src/index.ts`
7. Event Worker (4.7) - `src/workers/eventWorker.ts`
8. Workers Entry Point (4.8) - Update `src/workers/index.ts`

Lass mich die Dokumentation lesen, um die Details zu verstehen.</details></think>

---

Gut, ich habe einen guten Überblick. Die Queues sind bereits implementiert. Jetzt lese ich die Dokumentation für die Details der Event Ingestion.

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\docs\DNA_ME_CRM_CURSOR_MASTER_PROMPT_PART2.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\docs\DNA_ME_CRM_CURSOR_MASTER_PROMPT_PART1.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\src\api\middleware\errorHandler.ts</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Jetzt habe ich einen guten Überblick. Ich muss die folgenden Dateien erstellen/aktualisieren für Etappe 4:

1. **API-Key Middleware** (`src/api/middleware/apiKey.ts`) - Validiert X-API-Key header
2. **HMAC Middleware** (`src/api/middleware/hmac.ts`) - Validiert X-Webhook-Signature mit HMAC-SHA256
3. **Event Validation Schema** (`src/api/schemas/events.ts`) - Zod schemas für Events
4. **Events Route** (`src/api/routes/events.ts`) - POST /events/ingest, POST /leads/bulk
5. **Event Worker** (`src/workers/eventWorker.ts`) - Prozessiert Events aus der Queue
6. **Aktualisiere** `src/index.ts` - Registriere Events Routes
7. **Aktualisiere** `src/workers/index.ts` - Füge Event Worker hinzu

Lassen Sie mich auch den zweiten Teil der Dokumentation lesen, um mehr Details über den Event Worker zu bekommen.</details></think>

---

Lassen Sie mich auch den Worker-Teil der Dokumentation lesen:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\docs\DNA_ME_CRM_CURSOR_MASTER_PROMPT_PART2.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\migrations\1737475200000_initial-schema.sql</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Jetzt habe ich alle Informationen, die ich brauche. Ich werde alle Dateien für Etappe 4 erstellen:

1. **API-Key Middleware** (`src/api/middleware/apiKey.ts`)
2. **HMAC Middleware** (`src/api/middleware/hmac.ts`)
3. **Event Validation Schema** (`src/api/schemas/events.ts`)
4. **Events Route** (`src/api/routes/events.ts`)
5. **Event Worker** (`src/workers/eventWorker.ts`)
6. **Update** `src/index.ts` - Registriere Events Routes
7. **Update** `src/workers/index.ts` - Füge Event Worker hinzu

Da der Event Worker auf den ScoringEngine, IntentDetector und AutomationEngine angewiesen ist (die in späteren Etappen implementiert werden), werde ich eine einfachere Version erstellen, die nur den Lead erstellt/findet, das Event speichert und die Attribution aktualisiert. Die Scoring/Intent/Automation-Integration kommt später.

Lassen Sie mich jetzt alle Dateien erstellen.</details></think>

---

Jetzt habe ich alle Informationen. Ich werde alle Komponenten für Etappe 4 erstellen:

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: E:\dname\crm_dna-me\src\api\middleware\apiKey.ts</summary>

**Chunk 1**
Lines added: 82, lines removed: 1

```diff
@@ -1,1 +1,82 @@
- 
+ // =============================================================================
+ // src/api/middleware/apiKey.ts
+ // API Key Authentication Middleware
+ // =============================================================================
+ 
+ import type { FastifyRequest, FastifyReply } from 'fastify';
+ import { config } from '../../config/index.js';
+ import { ApiKeyError } from '../../errors/index.js';
+ 
+ // =============================================================================
+ // Types
+ // =============================================================================
+ 
+ declare module 'fastify' {
+   interface FastifyRequest {
+     apiKeySource?: string;
+   }
+ }
+ 
+ // =============================================================================
+ // API Key Validation
+ // =============================================================================
+ 
+ /**
+  * Validates the X-API-Key header against configured API keys.
+  * Attaches the source to the request for audit logging.
+  */
+ export async function validateApiKey(
+   request: FastifyRequest,
+   reply: FastifyReply
+ ): Promise&lt;void&gt; {
+   const apiKey = request.headers['x-api-key'];
+ 
+   if (!apiKey) {
+     throw new ApiKeyError('Missing X-API-Key header');
+   }
+ 
+   if (Array.isArray(apiKey)) {
+     throw new ApiKeyError('Multiple X-API-Key headers not allowed');
+   }
+ 
+   // Find matching API key in configuration
+   const keyConfig = config.apiKeys.find(k =&gt; k.key === apiKey);
+ 
+   if (!keyConfig) {
+     request.log.warn({ apiKey: apiKey.slice(0, 8) + '...' }, 'Invalid API key attempt');
+     throw new ApiKeyError('Invalid API key');
+   }
+ 
+   // Attach source to request for downstream use
+   request.apiKeySource = keyConfig.source;
+ 
+   request.log.debug({ source: keyConfig.source }, 'API key validated');
+ }
+ 
+ // =============================================================================
+ // Optional API Key (for endpoints that work with or without auth)
+ // =============================================================================
+ 
+ export async function optionalApiKey(
+   request: FastifyRequest,
+   reply: FastifyReply
+ ): Promise&lt;void&gt; {
+   const apiKey = request.headers['x-api-key'];
+ 
+   if (!apiKey) {
+     return; // No key provided, continue without source
+   }
+ 
+   if (Array.isArray(apiKey)) {
+     return; // Invalid format, continue without source
+   }
+ 
+   const keyConfig = config.apiKeys.find(k =&gt; k.key === apiKey);
+ 
+   if (keyConfig) {
+     request.apiKeySource = keyConfig.source;
+   }
+ }
+ 
+ export default validateApiKey;
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: E:\dname\crm_dna-me\src\api\middleware\hmac.ts</summary>

**Chunk 1**
Lines added: 129, lines removed: 1

```diff
@@ -1,1 +1,129 @@
- 
+ // =============================================================================
+ // src/api/middleware/hmac.ts
+ // HMAC Signature Validation Middleware for Webhooks
+ // =============================================================================
+ 
+ import { createHmac, timingSafeEqual } from 'crypto';
+ import type { FastifyRequest, FastifyReply } from 'fastify';
+ import { config } from '../../config/index.js';
+ import { HmacSignatureError } from '../../errors/index.js';
+ 
+ // =============================================================================
+ // Constants
+ // =============================================================================
+ 
+ const SIGNATURE_HEADER = 'x-webhook-signature';
+ const TIMESTAMP_HEADER = 'x-webhook-timestamp';
+ const SIGNATURE_ALGORITHM = 'sha256';
+ const MAX_TIMESTAMP_AGE_SECONDS = 300; // 5 minutes
+ 
+ // =============================================================================
+ // HMAC Validation
+ // =============================================================================
+ 
+ /**
+  * Validates the X-Webhook-Signature header using HMAC-SHA256.
+  * Uses timing-safe comparison to prevent timing attacks.
+  * 
+  * Expected signature format: sha256=&lt;hex_digest&gt;
+  * Payload signed: timestamp.body
+  */
+ export async function validateHmacSignature(
+   request: FastifyRequest,
+   reply: FastifyReply
+ ): Promise&lt;void&gt; {
+   const signature = request.headers[SIGNATURE_HEADER];
+   const timestamp = request.headers[TIMESTAMP_HEADER];
+ 
+   // Check for signature header
+   if (!signature) {
+     throw new HmacSignatureError('Missing X-Webhook-Signature header');
+   }
+ 
+   if (Array.isArray(signature)) {
+     throw new HmacSignatureError('Multiple signature headers not allowed');
+   }
+ 
+   // Parse signature format
+   const signatureParts = signature.split('=');
+   if (signatureParts.length !== 2 || signatureParts[0] !== 'sha256') {
+     throw new HmacSignatureError('Invalid signature format. Expected: sha256=&lt;signature&gt;');
+   }
+ 
+   const providedSignature = signatureParts[1];
+ 
+   // Validate timestamp if provided (replay attack prevention)
+   if (timestamp &amp;&amp; !Array.isArray(timestamp)) {
+     const timestampSeconds = parseInt(timestamp, 10);
+     const currentSeconds = Math.floor(Date.now() / 1000);
+     const age = currentSeconds - timestampSeconds;
+ 
+     if (isNaN(timestampSeconds) || age &gt; MAX_TIMESTAMP_AGE_SECONDS) {
+       request.log.warn({ age, maxAge: MAX_TIMESTAMP_AGE_SECONDS }, 'Webhook timestamp too old');
+       throw new HmacSignatureError('Webhook timestamp expired');
+     }
+   }
+ 
+   // Get raw body for signature verification
+   const body = request.body;
+   const bodyString = typeof body === 'string' ? body : JSON.stringify(body);
+ 
+   // Build payload to sign
+   const payload = timestamp ? &#96;${timestamp}.${bodyString}&#96; : bodyString;
+ 
+   // Calculate expected signature
+   const expectedSignature = createHmac(SIGNATURE_ALGORITHM, config.webhookSecret)
+     .update(payload)
+     .digest('hex');
+ 
+   // Timing-safe comparison
+   const providedBuffer = Buffer.from(providedSignature, 'hex');
+   const expectedBuffer = Buffer.from(expectedSignature, 'hex');
+ 
+   // Ensure buffers have same length before comparison
+   if (providedBuffer.length !== expectedBuffer.length) {
+     request.log.warn('Webhook signature length mismatch');
+     throw new HmacSignatureError('Invalid webhook signature');
+   }
+ 
+   if (!timingSafeEqual(providedBuffer, expectedBuffer)) {
+     request.log.warn('Webhook signature verification failed');
+     throw new HmacSignatureError('Invalid webhook signature');
+   }
+ 
+   request.log.debug('Webhook signature validated');
+ }
+ 
+ // =============================================================================
+ // Helper: Generate Signature (for testing)
+ // =============================================================================
+ 
+ /**
+  * Generates an HMAC signature for a payload.
+  * Useful for testing webhook integrations.
+  */
+ export function generateSignature(payload: string, secret?: string): string {
+   const hmac = createHmac(SIGNATURE_ALGORITHM, secret || config.webhookSecret);
+   return &#96;sha256=${hmac.update(payload).digest('hex')}&#96;;
+ }
+ 
+ /**
+  * Generates signature with timestamp.
+  */
+ export function generateSignatureWithTimestamp(
+   payload: string,
+   timestamp?: number,
+   secret?: string
+ ): { signature: string; timestamp: string } {
+   const ts = timestamp || Math.floor(Date.now() / 1000);
+   const payloadWithTimestamp = &#96;${ts}.${payload}&#96;;
+   const hmac = createHmac(SIGNATURE_ALGORITHM, secret || config.webhookSecret);
+   
+   return {
+     signature: &#96;sha256=${hmac.update(payloadWithTimestamp).digest('hex')}&#96;,
+     timestamp: String(ts)
+   };
+ }
+ 
+ export default validateHmacSignature;
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: E:\dname\crm_dna-me\src\api\schemas\events.ts</summary>

**Chunk 1**
Lines added: 207, lines removed: 1

```diff
@@ -1,1 +1,207 @@
- 
+ // =============================================================================
+ // src/api/schemas/events.ts
+ // Zod Validation Schemas for Event Ingestion
+ // =============================================================================
+ 
+ import { z } from 'zod';
+ 
+ // =============================================================================
+ // Lead Identifier Schema
+ // =============================================================================
+ 
+ /**
+  * At least one identifier is required to match or create a lead.
+  * Priority: email &gt; portal_id &gt; waalaxy_id &gt; linkedin_url &gt; lemlist_id
+  */
+ export const leadIdentifierSchema = z.object({
+   email: z.string().email().optional(),
+   portal_id: z.string().optional(),
+   waalaxy_id: z.string().optional(),
+   linkedin_url: z.string().url().optional(),
+   lemlist_id: z.string().optional()
+ }).refine(
+   (data) =&gt; data.email || data.portal_id || data.waalaxy_id || data.linkedin_url || data.lemlist_id,
+   { message: 'At least one lead identifier is required (email, portal_id, waalaxy_id, linkedin_url, or lemlist_id)' }
+ );
+ 
+ export type LeadIdentifier = z.infer&lt;typeof leadIdentifierSchema&gt;;
+ 
+ // =============================================================================
+ // Event Metadata Schema
+ // =============================================================================
+ 
+ /**
+  * Flexible metadata object for event-specific data.
+  * Can contain any JSON-serializable value.
+  */
+ export const eventMetadataSchema = z.record(z.unknown()).default({});
+ 
+ // =============================================================================
+ // Ingest Event Schema
+ // =============================================================================
+ 
+ /**
+  * Schema for the POST /events/ingest endpoint.
+  */
+ export const ingestEventSchema = z.object({
+   // Event Type (required)
+   event_type: z.string().min(1).max(100),
+   
+   // Event Category (optional, for grouping)
+   event_category: z.string().max(50).optional(),
+   
+   // Source of the event (required)
+   source: z.enum([
+     'waalaxy',
+     'portal', 
+     'lemlist',
+     'ads',
+     'conference',
+     'website',
+     'linkedin',
+     'manual',
+     'api',
+     'import'
+   ]),
+   
+   // When the event occurred (required, ISO 8601)
+   occurred_at: z.string().datetime({ message: 'occurred_at must be a valid ISO 8601 datetime' }),
+   
+   // Lead identifier (required)
+   lead_identifier: leadIdentifierSchema,
+   
+   // Event metadata (optional)
+   metadata: eventMetadataSchema,
+   
+   // Campaign tracking (optional)
+   campaign_id: z.string().max(100).optional(),
+   utm_source: z.string().max(100).optional(),
+   utm_medium: z.string().max(100).optional(),
+   utm_campaign: z.string().max(100).optional(),
+   
+   // Correlation ID for tracking related events (optional)
+   correlation_id: z.string().uuid().optional()
+ });
+ 
+ export type IngestEventInput = z.infer&lt;typeof ingestEventSchema&gt;;
+ 
+ // =============================================================================
+ // Ingest Response Schema
+ // =============================================================================
+ 
+ export const ingestEventResponseSchema = z.object({
+   success: z.boolean(),
+   event_id: z.string().uuid(),
+   message: z.string(),
+   queued_at: z.string().datetime()
+ });
+ 
+ export type IngestEventResponse = z.infer&lt;typeof ingestEventResponseSchema&gt;;
+ 
+ // =============================================================================
+ // Bulk Import Schema
+ // =============================================================================
+ 
+ /**
+  * Schema for individual lead in bulk import.
+  */
+ export const bulkLeadSchema = z.object({
+   email: z.string().email(),
+   first_name: z.string().max(100).optional(),
+   last_name: z.string().max(100).optional(),
+   phone: z.string().max(50).optional(),
+   job_title: z.string().max(150).optional(),
+   company_name: z.string().max(255).optional(),
+   company_domain: z.string().max(255).optional(),
+   linkedin_url: z.string().url().optional(),
+   source: z.string().max(100).optional(),
+   campaign_id: z.string().max(100).optional(),
+   metadata: eventMetadataSchema
+ });
+ 
+ export type BulkLeadInput = z.infer&lt;typeof bulkLeadSchema&gt;;
+ 
+ /**
+  * Schema for the POST /leads/bulk endpoint.
+  */
+ export const bulkImportSchema = z.object({
+   leads: z.array(bulkLeadSchema).min(1).max(1000),
+   source: z.string().min(1).max(100),
+   campaign_id: z.string().max(100).optional(),
+   skip_duplicates: z.boolean().default(true),
+   notify_on_complete: z.boolean().default(false)
+ });
+ 
+ export type BulkImportInput = z.infer&lt;typeof bulkImportSchema&gt;;
+ 
+ /**
+  * Bulk import response schema.
+  */
+ export const bulkImportResponseSchema = z.object({
+   success: z.boolean(),
+   message: z.string(),
+   batch_id: z.string().uuid(),
+   total_leads: z.number(),
+   queued_at: z.string().datetime()
+ });
+ 
+ export type BulkImportResponse = z.infer&lt;typeof bulkImportResponseSchema&gt;;
+ 
+ // =============================================================================
+ // Event Types (for reference)
+ // =============================================================================
+ 
+ /**
+  * Common event types used in the system.
+  * This is not a strict enum - custom event types are allowed.
+  */
+ export const COMMON_EVENT_TYPES = [
+   // Website Events
+   'page_visited',
+   'form_submitted',
+   'demo_requested',
+   'pricing_viewed',
+   'roi_calculator_submitted',
+   'download_completed',
+   'video_watched',
+   
+   // Email Events
+   'email_sent',
+   'email_opened',
+   'email_clicked',
+   'email_replied',
+   'email_bounced',
+   'email_unsubscribed',
+   
+   // LinkedIn Events
+   'linkedin_connection_sent',
+   'linkedin_connection_accepted',
+   'linkedin_message_sent',
+   'linkedin_message_replied',
+   'linkedin_profile_viewed',
+   
+   // Sales Events
+   'meeting_scheduled',
+   'meeting_completed',
+   'call_completed',
+   'proposal_sent',
+   'contract_sent',
+   'deal_won',
+   'deal_lost',
+   
+   // Product Events
+   'user_registered',
+   'order_placed',
+   'order_completed',
+   'subscription_started',
+   'subscription_cancelled',
+   
+   // Marketing Events
+   'webinar_registered',
+   'webinar_attended',
+   'conference_visited',
+   'trade_show_scanned'
+ ] as const;
+ 
+ export type CommonEventType = typeof COMMON_EVENT_TYPES[number];
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: E:\dname\crm_dna-me\src\api\routes\events.ts</summary>

**Chunk 1**
Lines added: 397, lines removed: 1

```diff
@@ -1,1 +1,397 @@
- 
+ // =============================================================================
+ // src/api/routes/events.ts
+ // Event Ingestion API Routes
+ // =============================================================================
+ 
+ import type { FastifyInstance, FastifyRequest, FastifyReply } from 'fastify';
+ import { v4 as uuidv4 } from 'uuid';
+ import { validateApiKey } from '../middleware/apiKey.js';
+ import { validateHmacSignature } from '../middleware/hmac.js';
+ import {
+   ingestEventSchema,
+   bulkImportSchema,
+   type IngestEventInput,
+   type IngestEventResponse,
+   type BulkImportInput,
+   type BulkImportResponse
+ } from '../schemas/events.js';
+ import { getEventsQueue } from '../../config/queues.js';
+ import { ValidationError } from '../../errors/index.js';
+ import type { EventProcessingJob } from '../../types/index.js';
+ 
+ // =============================================================================
+ // Route Registration
+ // =============================================================================
+ 
+ export async function eventsRoutes(fastify: FastifyInstance): Promise&lt;void&gt; {
+   
+   // ===========================================================================
+   // POST /api/v1/events/ingest
+   // ===========================================================================
+   /**
+    * Ingest a single event from external sources.
+    * Validates the event and queues it for async processing.
+    * 
+    * Authentication: X-API-Key header required
+    * Response: 202 Accepted (event queued for processing)
+    */
+   fastify.post&lt;{
+     Body: IngestEventInput;
+     Reply: IngestEventResponse;
+   }&gt;(
+     '/events/ingest',
+     {
+       preHandler: validateApiKey,
+       schema: {
+         description: 'Ingest a marketing event for processing',
+         tags: ['Events'],
+         body: {
+           type: 'object',
+           required: ['event_type', 'source', 'occurred_at', 'lead_identifier'],
+           properties: {
+             event_type: { type: 'string' },
+             event_category: { type: 'string' },
+             source: { type: 'string', enum: ['waalaxy', 'portal', 'lemlist', 'ads', 'conference', 'website', 'linkedin', 'manual', 'api', 'import'] },
+             occurred_at: { type: 'string', format: 'date-time' },
+             lead_identifier: {
+               type: 'object',
+               properties: {
+                 email: { type: 'string', format: 'email' },
+                 portal_id: { type: 'string' },
+                 waalaxy_id: { type: 'string' },
+                 linkedin_url: { type: 'string', format: 'uri' },
+                 lemlist_id: { type: 'string' }
+               }
+             },
+             metadata: { type: 'object', additionalProperties: true },
+             campaign_id: { type: 'string' },
+             utm_source: { type: 'string' },
+             utm_medium: { type: 'string' },
+             utm_campaign: { type: 'string' },
+             correlation_id: { type: 'string', format: 'uuid' }
+           }
+         },
+         response: {
+           202: {
+             type: 'object',
+             properties: {
+               success: { type: 'boolean' },
+               event_id: { type: 'string', format: 'uuid' },
+               message: { type: 'string' },
+               queued_at: { type: 'string', format: 'date-time' }
+             }
+           }
+         }
+       }
+     },
+     async (request, reply) =&gt; {
+       // Validate request body with Zod
+       const parseResult = ingestEventSchema.safeParse(request.body);
+       
+       if (!parseResult.success) {
+         throw new ValidationError('Invalid event data', {
+           validationErrors: parseResult.error.errors
+         });
+       }
+ 
+       const eventData = parseResult.data;
+       const eventId = uuidv4();
+       const queuedAt = new Date().toISOString();
+ 
+       // Build job data
+       const job: EventProcessingJob = {
+         event_id: eventId,
+         event_type: eventData.event_type,
+         source: eventData.source,
+         lead_identifier: eventData.lead_identifier,
+         metadata: {
+           ...eventData.metadata,
+           event_category: eventData.event_category,
+           campaign_id: eventData.campaign_id,
+           utm_source: eventData.utm_source,
+           utm_medium: eventData.utm_medium,
+           utm_campaign: eventData.utm_campaign,
+           correlation_id: eventData.correlation_id,
+           api_key_source: request.apiKeySource
+         },
+         occurred_at: eventData.occurred_at
+       };
+ 
+       // Queue the event for processing
+       const queue = getEventsQueue();
+       await queue.add('process_event', job, {
+         jobId: eventId,
+         priority: getPriorityForEventType(eventData.event_type),
+         attempts: 3,
+         backoff: {
+           type: 'exponential',
+           delay: 1000
+         }
+       });
+ 
+       request.log.info({
+         eventId,
+         eventType: eventData.event_type,
+         source: eventData.source,
+         leadEmail: eventData.lead_identifier.email
+       }, 'Event queued for processing');
+ 
+       return reply.code(202).send({
+         success: true,
+         event_id: eventId,
+         message: 'Event queued for processing',
+         queued_at: queuedAt
+       });
+     }
+   );
+ 
+   // ===========================================================================
+   // POST /api/v1/events/webhook
+   // ===========================================================================
+   /**
+    * Generic webhook endpoint for external services.
+    * Uses HMAC signature validation.
+    */
+   fastify.post&lt;{
+     Body: IngestEventInput;
+     Reply: IngestEventResponse;
+   }&gt;(
+     '/events/webhook',
+     {
+       preHandler: validateHmacSignature,
+       schema: {
+         description: 'Receive events via webhook (HMAC validated)',
+         tags: ['Events']
+       }
+     },
+     async (request, reply) =&gt; {
+       // Same processing as /events/ingest
+       const parseResult = ingestEventSchema.safeParse(request.body);
+       
+       if (!parseResult.success) {
+         throw new ValidationError('Invalid webhook payload', {
+           validationErrors: parseResult.error.errors
+         });
+       }
+ 
+       const eventData = parseResult.data;
+       const eventId = uuidv4();
+       const queuedAt = new Date().toISOString();
+ 
+       const job: EventProcessingJob = {
+         event_id: eventId,
+         event_type: eventData.event_type,
+         source: eventData.source,
+         lead_identifier: eventData.lead_identifier,
+         metadata: {
+           ...eventData.metadata,
+           event_category: eventData.event_category,
+           campaign_id: eventData.campaign_id,
+           utm_source: eventData.utm_source,
+           utm_medium: eventData.utm_medium,
+           utm_campaign: eventData.utm_campaign,
+           correlation_id: eventData.correlation_id,
+           webhook: true
+         },
+         occurred_at: eventData.occurred_at
+       };
+ 
+       const queue = getEventsQueue();
+       await queue.add('process_event', job, {
+         jobId: eventId,
+         priority: getPriorityForEventType(eventData.event_type)
+       });
+ 
+       request.log.info({
+         eventId,
+         eventType: eventData.event_type,
+         source: eventData.source
+       }, 'Webhook event queued');
+ 
+       return reply.code(202).send({
+         success: true,
+         event_id: eventId,
+         message: 'Webhook event queued for processing',
+         queued_at: queuedAt
+       });
+     }
+   );
+ 
+   // ===========================================================================
+   // POST /api/v1/leads/bulk
+   // ===========================================================================
+   /**
+    * Bulk import leads from CSV or external systems.
+    * Creates events for each lead.
+    */
+   fastify.post&lt;{
+     Body: BulkImportInput;
+     Reply: BulkImportResponse;
+   }&gt;(
+     '/leads/bulk',
+     {
+       preHandler: validateApiKey,
+       schema: {
+         description: 'Bulk import leads',
+         tags: ['Leads', 'Events'],
+         body: {
+           type: 'object',
+           required: ['leads', 'source'],
+           properties: {
+             leads: {
+               type: 'array',
+               items: {
+                 type: 'object',
+                 required: ['email'],
+                 properties: {
+                   email: { type: 'string', format: 'email' },
+                   first_name: { type: 'string' },
+                   last_name: { type: 'string' },
+                   phone: { type: 'string' },
+                   job_title: { type: 'string' },
+                   company_name: { type: 'string' },
+                   company_domain: { type: 'string' },
+                   linkedin_url: { type: 'string', format: 'uri' },
+                   source: { type: 'string' },
+                   campaign_id: { type: 'string' },
+                   metadata: { type: 'object', additionalProperties: true }
+                 }
+               },
+               minItems: 1,
+               maxItems: 1000
+             },
+             source: { type: 'string' },
+             campaign_id: { type: 'string' },
+             skip_duplicates: { type: 'boolean', default: true },
+             notify_on_complete: { type: 'boolean', default: false }
+           }
+         },
+         response: {
+           202: {
+             type: 'object',
+             properties: {
+               success: { type: 'boolean' },
+               message: { type: 'string' },
+               batch_id: { type: 'string', format: 'uuid' },
+               total_leads: { type: 'number' },
+               queued_at: { type: 'string', format: 'date-time' }
+             }
+           }
+         }
+       }
+     },
+     async (request, reply) =&gt; {
+       // Validate request body
+       const parseResult = bulkImportSchema.safeParse(request.body);
+       
+       if (!parseResult.success) {
+         throw new ValidationError('Invalid bulk import data', {
+           validationErrors: parseResult.error.errors
+         });
+       }
+ 
+       const bulkData = parseResult.data;
+       const batchId = uuidv4();
+       const queuedAt = new Date().toISOString();
+       const queue = getEventsQueue();
+ 
+       // Queue individual events for each lead
+       const jobs = bulkData.leads.map((lead, index) =&gt; {
+         const eventId = uuidv4();
+         
+         const job: EventProcessingJob = {
+           event_id: eventId,
+           event_type: 'lead_imported',
+           source: bulkData.source as 'import',
+           lead_identifier: {
+             email: lead.email,
+             linkedin_url: lead.linkedin_url
+           },
+           metadata: {
+             ...lead.metadata,
+             first_name: lead.first_name,
+             last_name: lead.last_name,
+             phone: lead.phone,
+             job_title: lead.job_title,
+             company_name: lead.company_name,
+             company_domain: lead.company_domain,
+             campaign_id: bulkData.campaign_id || lead.campaign_id,
+             batch_id: batchId,
+             batch_index: index,
+             batch_total: bulkData.leads.length,
+             skip_duplicates: bulkData.skip_duplicates,
+             api_key_source: request.apiKeySource
+           },
+           occurred_at: queuedAt
+         };
+ 
+         return {
+           name: 'process_event',
+           data: job,
+           opts: {
+             jobId: eventId,
+             priority: 10 // Lower priority for bulk imports
+           }
+         };
+       });
+ 
+       // Bulk add jobs to queue
+       await queue.addBulk(jobs);
+ 
+       request.log.info({
+         batchId,
+         totalLeads: bulkData.leads.length,
+         source: bulkData.source
+       }, 'Bulk import queued');
+ 
+       return reply.code(202).send({
+         success: true,
+         message: &#96;${bulkData.leads.length} leads queued for import&#96;,
+         batch_id: batchId,
+         total_leads: bulkData.leads.length,
+         queued_at: queuedAt
+       });
+     }
+   );
+ }
+ 
+ // =============================================================================
+ // Helper Functions
+ // =============================================================================
+ 
+ /**
+  * Determines job priority based on event type.
+  * Lower number = higher priority.
+  */
+ function getPriorityForEventType(eventType: string): number {
+   // High priority events (immediate response expected)
+   const highPriority = [
+     'demo_requested',
+     'form_submitted',
+     'meeting_scheduled',
+     'order_placed',
+     'deal_won'
+   ];
+ 
+   // Medium priority events
+   const mediumPriority = [
+     'email_replied',
+     'linkedin_message_replied',
+     'pricing_viewed',
+     'roi_calculator_submitted'
+   ];
+ 
+   if (highPriority.includes(eventType)) {
+     return 1;
+   }
+   
+   if (mediumPriority.includes(eventType)) {
+     return 5;
+   }
+ 
+   // Default priority
+   return 10;
+ }
+ 
+ export default eventsRoutes;
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: E:\dname\crm_dna-me\src\workers\eventWorker.ts</summary>

**Chunk 1**
Lines added: 395, lines removed: 1

```diff
@@ -1,1 +1,395 @@
- 
+ // =============================================================================
+ // src/workers/eventWorker.ts
+ // BullMQ Worker for Processing Marketing Events
+ // =============================================================================
+ 
+ import { Worker, Job } from 'bullmq';
+ import { redisOptions } from '../config/redis.js';
+ import { QUEUE_NAMES } from '../config/queues.js';
+ import { db } from '../db/index.js';
+ import type { EventProcessingJob, Lead, MarketingEvent, LeadIdentifier } from '../types/index.js';
+ 
+ // =============================================================================
+ // Worker Factory
+ // =============================================================================
+ 
+ export function createEventWorker(): Worker&lt;EventProcessingJob&gt; {
+   const worker = new Worker&lt;EventProcessingJob&gt;(
+     QUEUE_NAMES.EVENTS,
+     async (job: Job&lt;EventProcessingJob&gt;) =&gt; {
+       const { event_id, event_type, source, lead_identifier, metadata, occurred_at } = job.data;
+       
+       console.log(&#96;[Event Worker] Processing ${event_id}: ${event_type} from ${source}&#96;);
+       
+       try {
+         // 1. Find or create lead
+         const lead = await findOrCreateLead(lead_identifier, source, metadata);
+         console.log(&#96;[Event Worker] Lead: ${lead.id} (${lead.email})&#96;);
+         
+         // 2. Store event
+         const event = await storeEvent({
+           id: event_id,
+           lead_id: lead.id,
+           event_type,
+           event_category: metadata.event_category as string,
+           source,
+           metadata,
+           campaign_id: metadata.campaign_id as string,
+           utm_source: metadata.utm_source as string,
+           utm_medium: metadata.utm_medium as string,
+           utm_campaign: metadata.utm_campaign as string,
+           correlation_id: metadata.correlation_id as string,
+           occurred_at: new Date(occurred_at)
+         });
+         console.log(&#96;[Event Worker] Event stored: ${event.id}&#96;);
+         
+         // 3. Update attribution
+         await updateAttribution(lead, event);
+         
+         // 4. Update lead's last activity
+         await db.execute(&#96;
+           UPDATE leads SET last_activity = NOW(), updated_at = NOW() WHERE id = $1
+         &#96;, [lead.id]);
+         
+         // 5. Mark event as processed
+         await db.execute(&#96;
+           UPDATE events SET processed_at = NOW() WHERE id = $1
+         &#96;, [event_id]);
+         
+         // NOTE: Scoring, Intent Detection, and Routing will be added in later phases
+         // These will be called here:
+         // - scoringEngine.processEvent(event, lead)
+         // - intentDetector.processEvent(event, lead)
+         // - routingQueue.add('evaluate', { lead_id: lead.id })
+         
+         return {
+           success: true,
+           lead_id: lead.id,
+           event_id: event.id,
+           event_type,
+           source
+         };
+         
+       } catch (error) {
+         console.error(&#96;[Event Worker] Error processing ${event_id}:&#96;, error);
+         throw error;
+       }
+     },
+     {
+       connection: redisOptions,
+       concurrency: 10,
+       limiter: {
+         max: 100,
+         duration: 1000
+       }
+     }
+   );
+   
+   // Event handlers
+   worker.on('completed', (job) =&gt; {
+     console.log(&#96;[Event Worker] ✅ Job ${job.id} completed&#96;);
+   });
+   
+   worker.on('failed', (job, error) =&gt; {
+     console.error(&#96;[Event Worker] ❌ Job ${job?.id} failed:&#96;, error.message);
+   });
+   
+   worker.on('error', (error) =&gt; {
+     console.error('[Event Worker] Worker error:', error);
+   });
+   
+   return worker;
+ }
+ 
+ // =============================================================================
+ // Helper Functions
+ // =============================================================================
+ 
+ /**
+  * Finds an existing lead by identifier or creates a new one.
+  * Priority: email &gt; portal_id &gt; waalaxy_id &gt; linkedin_url &gt; lemlist_id
+  */
+ async function findOrCreateLead(
+   identifier: LeadIdentifier,
+   source: string,
+   metadata: Record&lt;string, unknown&gt;
+ ): Promise&lt;Lead&gt; {
+   // Try to find by email first (most reliable)
+   if (identifier.email) {
+     const existing = await db.queryOne&lt;Lead&gt;(&#96;
+       SELECT * FROM leads WHERE email = $1
+     &#96;, [identifier.email]);
+     
+     if (existing) {
+       await updateLeadIdentifiers(existing.id, identifier);
+       await updateLeadFromMetadata(existing.id, metadata);
+       return existing;
+     }
+   }
+   
+   // Try other identifiers
+   const identifierFields = ['portal_id', 'waalaxy_id', 'linkedin_url', 'lemlist_id'] as const;
+   
+   for (const field of identifierFields) {
+     const value = identifier[field];
+     if (value) {
+       const existing = await db.queryOne&lt;Lead&gt;(&#96;
+         SELECT * FROM leads WHERE ${field} = $1
+       &#96;, [value]);
+       
+       if (existing) {
+         await updateLeadIdentifiers(existing.id, identifier);
+         await updateLeadFromMetadata(existing.id, metadata);
+         return existing;
+       }
+     }
+   }
+   
+   // Create new lead
+   const email = identifier.email || &#96;unknown+${Date.now()}@temp.dna-me.com&#96;;
+   
+   const newLead = await db.queryOne&lt;Lead&gt;(&#96;
+     INSERT INTO leads (
+       email, 
+       first_name,
+       last_name,
+       phone,
+       job_title,
+       portal_id, 
+       waalaxy_id, 
+       linkedin_url,
+       lemlist_id,
+       status, 
+       lifecycle_stage, 
+       routing_status,
+       first_touch_source, 
+       first_touch_date
+     ) VALUES (
+       $1, $2, $3, $4, $5, $6, $7, $8, $9,
+       'new', 'lead', 'unrouted',
+       $10, NOW()
+     )
+     RETURNING *
+   &#96;, [
+     email,
+     metadata.first_name || null,
+     metadata.last_name || null,
+     metadata.phone || null,
+     metadata.job_title || null,
+     identifier.portal_id || null,
+     identifier.waalaxy_id || null,
+     identifier.linkedin_url || null,
+     identifier.lemlist_id || null,
+     source
+   ]);
+   
+   if (!newLead) {
+     throw new Error('Failed to create lead');
+   }
+   
+   console.log(&#96;[Event Worker] Created new lead: ${newLead.id} (${newLead.email})&#96;);
+   
+   // Create organization if company info provided
+   if (metadata.company_name || metadata.company_domain) {
+     await createOrLinkOrganization(newLead.id, metadata);
+   }
+   
+   return newLead;
+ }
+ 
+ /**
+  * Updates lead's external identifiers if not already set.
+  */
+ async function updateLeadIdentifiers(
+   leadId: string, 
+   identifiers: LeadIdentifier
+ ): Promise&lt;void&gt; {
+   const updates: string[] = [];
+   const values: unknown[] = [];
+   let paramIndex = 1;
+   
+   const fields = ['portal_id', 'waalaxy_id', 'linkedin_url', 'lemlist_id'] as const;
+   
+   for (const field of fields) {
+     const value = identifiers[field];
+     if (value) {
+       updates.push(&#96;${field} = COALESCE(${field}, $${paramIndex})&#96;);
+       values.push(value);
+       paramIndex++;
+     }
+   }
+   
+   if (updates.length &gt; 0) {
+     values.push(leadId);
+     await db.execute(&#96;
+       UPDATE leads SET ${updates.join(', ')}, updated_at = NOW()
+       WHERE id = $${paramIndex}
+     &#96;, values);
+   }
+ }
+ 
+ /**
+  * Updates lead fields from event metadata if not already set.
+  */
+ async function updateLeadFromMetadata(
+   leadId: string,
+   metadata: Record&lt;string, unknown&gt;
+ ): Promise&lt;void&gt; {
+   const updates: string[] = [];
+   const values: unknown[] = [];
+   let paramIndex = 1;
+   
+   const fieldMap = {
+     first_name: 'first_name',
+     last_name: 'last_name',
+     phone: 'phone',
+     job_title: 'job_title'
+   } as const;
+   
+   for (const [metaKey, dbField] of Object.entries(fieldMap)) {
+     const value = metadata[metaKey];
+     if (value &amp;&amp; typeof value === 'string') {
+       updates.push(&#96;${dbField} = COALESCE(${dbField}, $${paramIndex})&#96;);
+       values.push(value);
+       paramIndex++;
+     }
+   }
+   
+   if (updates.length &gt; 0) {
+     values.push(leadId);
+     await db.execute(&#96;
+       UPDATE leads SET ${updates.join(', ')}, updated_at = NOW()
+       WHERE id = $${paramIndex}
+     &#96;, values);
+   }
+ }
+ 
+ /**
+  * Creates or links an organization to a lead.
+  */
+ async function createOrLinkOrganization(
+   leadId: string,
+   metadata: Record&lt;string, unknown&gt;
+ ): Promise&lt;void&gt; {
+   const companyName = metadata.company_name as string;
+   const companyDomain = metadata.company_domain as string;
+   
+   if (!companyName &amp;&amp; !companyDomain) return;
+   
+   // Try to find existing organization by domain
+   let org = null;
+   if (companyDomain) {
+     org = await db.queryOne&lt;{ id: string }&gt;(&#96;
+       SELECT id FROM organizations WHERE domain = $1
+     &#96;, [companyDomain]);
+   }
+   
+   // Create organization if not found
+   if (!org &amp;&amp; companyName) {
+     org = await db.queryOne&lt;{ id: string }&gt;(&#96;
+       INSERT INTO organizations (name, domain)
+       VALUES ($1, $2)
+       ON CONFLICT (portal_id) DO NOTHING
+       RETURNING id
+     &#96;, [companyName, companyDomain || null]);
+   }
+   
+   // Link to lead
+   if (org) {
+     await db.execute(&#96;
+       UPDATE leads SET organization_id = $1, updated_at = NOW()
+       WHERE id = $2 AND organization_id IS NULL
+     &#96;, [org.id, leadId]);
+   }
+ }
+ 
+ /**
+  * Stores a marketing event in the database.
+  */
+ async function storeEvent(event: {
+   id: string;
+   lead_id: string;
+   event_type: string;
+   event_category?: string;
+   source: string;
+   metadata: Record&lt;string, unknown&gt;;
+   campaign_id?: string;
+   utm_source?: string;
+   utm_medium?: string;
+   utm_campaign?: string;
+   correlation_id?: string;
+   occurred_at: Date;
+ }): Promise&lt;MarketingEvent&gt; {
+   // Clean metadata to remove fields stored in dedicated columns
+   const cleanMetadata = { ...event.metadata };
+   delete cleanMetadata.event_category;
+   delete cleanMetadata.campaign_id;
+   delete cleanMetadata.utm_source;
+   delete cleanMetadata.utm_medium;
+   delete cleanMetadata.utm_campaign;
+   delete cleanMetadata.correlation_id;
+   delete cleanMetadata.api_key_source;
+   delete cleanMetadata.first_name;
+   delete cleanMetadata.last_name;
+   delete cleanMetadata.phone;
+   delete cleanMetadata.job_title;
+   delete cleanMetadata.company_name;
+   delete cleanMetadata.company_domain;
+   
+   const result = await db.queryOne&lt;MarketingEvent&gt;(&#96;
+     INSERT INTO events (
+       id, lead_id, event_type, event_category, source, 
+       metadata, campaign_id, utm_source, utm_medium, utm_campaign,
+       correlation_id, occurred_at
+     )
+     VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
+     RETURNING *
+   &#96;, [
+     event.id,
+     event.lead_id,
+     event.event_type,
+     event.event_category || null,
+     event.source,
+     JSON.stringify(cleanMetadata),
+     event.campaign_id || null,
+     event.utm_source || null,
+     event.utm_medium || null,
+     event.utm_campaign || null,
+     event.correlation_id || null,
+     event.occurred_at
+   ]);
+   
+   if (!result) {
+     throw new Error('Failed to store event');
+   }
+   
+   return result;
+ }
+ 
+ /**
+  * Updates lead attribution (first touch / last touch).
+  */
+ async function updateAttribution(lead: Lead, event: MarketingEvent): Promise&lt;void&gt; {
+   // Always update last touch
+   await db.execute(&#96;
+     UPDATE leads SET
+       last_touch_source = $1,
+       last_touch_campaign = $2,
+       last_touch_date = $3
+     WHERE id = $4
+   &#96;, [event.source, event.utm_campaign || null, event.occurred_at, lead.id]);
+   
+   // First touch only if not set
+   if (!lead.first_touch_source) {
+     await db.execute(&#96;
+       UPDATE leads SET
+         first_touch_source = $1,
+         first_touch_campaign = $2,
+         first_touch_date = $3
+       WHERE id = $4 AND first_touch_source IS NULL
+     &#96;, [event.source, event.utm_campaign || null, event.occurred_at, lead.id]);
+   }
+ }
+ 
+ export default createEventWorker;
+ 
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
Gut, ich habe alle neuen Dateien erstellt. Jetzt muss ich:
1. `src/index.ts` aktualisieren, um die Event-Routes zu registrieren
2. `src/workers/index.ts` aktualisieren, um den Event Worker zu starten
3. Die Linter-Fehler prüfenLassen Sie mich jetzt die bestehenden Dateien aktualisieren.</details></think>

---

Jetzt aktualisiere ich die Hauptdateien, um die Routes und den Worker zu integrieren:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: E:\dname\crm_dna-me\src\index.ts</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -11,4 +11,5 @@
  import { initializeQueues, closeQueues } from './config/queues.js';
  import { errorHandler, notFoundHandler } from './api/middleware/errorHandler.js';
+ import { eventsRoutes } from './api/routes/events.js';
  import type { HealthCheckResponse } from './types/index.js';
  
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: E:\dname\crm_dna-me\src\index.ts</summary>

**Chunk 1**
Lines added: 6, lines removed: undefined

```diff
@@ -118,4 +118,10 @@
    };
  });
+ 
+ // =============================================================================
+ // API Routes
+ // =============================================================================
+ 
+ await fastify.register(eventsRoutes, { prefix: '/api/v1' });
  
  // =============================================================================
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: E:\dname\crm_dna-me\src\workers\index.ts</summary>

**Chunk 1**
Lines added: 2, lines removed: undefined

```diff
@@ -4,7 +4,9 @@
  // =============================================================================
  
+ import { Worker } from 'bullmq';
  import { config } from '../config/index.js';
  import { getRedisConnection, closeRedisConnection } from '../config/redis.js';
  import { closePool } from '../db/index.js';
+ import { createEventWorker } from './eventWorker.js';
  
  console.log(&#96;
```

**Chunk 2**
Lines added: 6, lines removed: undefined

```diff
@@ -17,4 +19,10 @@
  ╚═══════════════════════════════════════════════════════════╝
  &#96;);
+ 
+ // =============================================================================
+ // Worker Registry
+ // =============================================================================
+ 
+ const workers: Worker[] = [];
  
  // =============================================================================
```

**Chunk 3**
Lines added: 8, lines removed: 3

```diff
@@ -29,11 +37,16 @@
    console.log('✅ Redis connected');
  
-   // Workers will be added in later phases:
-   // - Event Worker (Phase 3)
+   // Event Worker - processes incoming marketing events
+   const eventWorker = createEventWorker();
+   workers.push(eventWorker);
+   console.log('✅ Event Worker started (concurrency: 10)');
+ 
+   // Future workers will be added in later phases:
    // - Routing Worker (Phase 6)
+   // - Scoring Worker (if needed)
    // - Decay Worker (Phase 4)
    // - Sync Worker (Phase 8)
  
-   console.log('✅ Workers initialized (placeholder for Phase 3+)');
+   console.log(&#96;✅ ${workers.length} worker(s) initialized&#96;);
    console.log('Workers are ready and waiting for jobs...');
  }
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: E:\dname\crm_dna-me\src\workers\index.ts</summary>

**Chunk 1**
Lines added: 5, lines removed: undefined

```diff
@@ -60,4 +60,9 @@
  
    try {
+     // Close all workers
+     console.log(&#96;Closing ${workers.length} worker(s)...&#96;);
+     await Promise.all(workers.map(worker =&gt; worker.close()));
+     console.log('✅ All workers closed');
+ 
      // Close Redis connection
      await closeRedisConnection();
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
Alle Dateien wurden aktualisiert. Jetzt sollte ich die Linter-Fehler prüfen und dann den Code testen.</details></think>

---

Jetzt prüfe ich auf Linter-Fehler:

---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 7 files</summary>
      
Lint paths:

- `E:\dname\crm_dna-me\src\api\middleware\apiKey.ts`
- `E:\dname\crm_dna-me\src\api\middleware\hmac.ts`
- `E:\dname\crm_dna-me\src\api\schemas\events.ts`
- `E:\dname\crm_dna-me\src\api\routes\events.ts`
- `E:\dname\crm_dna-me\src\workers\eventWorker.ts`
- `E:\dname\crm_dna-me\src\index.ts`
- `E:\dname\crm_dna-me\src\workers\index.ts`

Lint results


**No lint errors found**
</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Keine Linter-Fehler. Jetzt sollte ich das System testen. Zuerst prüfe ich, ob die Docker-Container laufen, und dann starte ich den Server und die Worker.</details></think>

---

Keine Linter-Fehler. Lassen Sie mich prüfen, ob Docker läuft und dann den Server testen:

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** • Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Es läuft bereits Docker und der Dev-Server. Ich sollte die laufenden Prozesse lesen, um den Status zu sehen.</details></think>

---

Der Server läuft bereits. Lassen Sie mich den aktuellen Status prüfen:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\2.txt</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Der Server läuft. Jetzt muss ich den Worker starten. Aber da ich neue Dateien hinzugefügt habe, muss ich möglicherweise den Server neu starten. Lassen Sie mich zunächst prüfen, ob der Server die neuen Routes erkannt hat, und dann den Worker starten.</details></think>

---

Der Server läuft. Da ich neue Dateien hinzugefügt habe, muss ich den Server neu starten und auch den Worker starten. Lassen Sie mich das tun:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: cd E:\dname\crm_dna-me && npm run dev</summary>

```bash
cd E:\dname\crm_dna-me && npm run dev
```
</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Ich starte den Server in einem neuen Terminal. Lassen Sie mich auf den Start warten und dann den Worker starten.</details></think>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: Start-Sleep -Seconds 3</summary>

```bash
Start-Sleep -Seconds 3
```
</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Gut. Jetzt prüfe ich den Server-Status und starte den Worker.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\4.txt</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: cd E:\dname\crm_dna-me && npm run dev:workers</summary>

```bash
cd E:\dname\crm_dna-me && npm run dev:workers
```
</details>
</tool-use>

---

PowerShell unterstützt `&&` nicht. Lassen Sie mich die Befehle anders ausführen:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: Set-Location E:\dname\crm_dna-me; npm run dev</summary>

```bash
Set-Location E:\dname\crm_dna-me; npm run dev
```
</details>
</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: Start-Sleep -Seconds 5; Get-Content "c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\6.txt" -Tail 30</summary>

```bash
Start-Sleep -Seconds 5; Get-Content "c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\6.txt" -Tail 30
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\6.txt</summary>

</details>
</tool-use>

---

Es scheint ein Fehler beim Starten aufgetreten zu sein. Ich muss herausfinden, was passiert:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: Get-Content "c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\6.txt"</summary>

```bash
Get-Content "c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\6.txt"
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\d6e788bc-8986-4351-b902-d2e1ee604562.txt</summary>

</details>
</tool-use>

---

Es gibt Encoding-Probleme mit dem kyrillischen Benutzernamen. Lassen Sie mich die Terminal-Datei direkt lesen:

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** • Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

Terminal 2 und 6 laufen. Ich sehe Terminal 2 läuft mit `npm run dev`. Lassen Sie mich das prüfen:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\2.txt</summary>

</details>
</tool-use>

---

Der Server läuft noch. Aber das ist die alte Version ohne die neuen Routes. Der neue Server sollte in Terminal 6 laufen. Lassen Sie mich das prüfen:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\6.txt</summary>

</details>
</tool-use>

---

Der Server konnte nicht starten (Port vermutlich bereits belegt). Das ist OK - Terminal 2 hat den alten Server laufen. Da tsx watch im Einsatz ist, werden Änderungen automatisch übernommen. Ich sehe aber, dass es einen Fehler gab. Lassen Sie mich den alten Server prüfen und die API testen:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: Invoke-RestMethod http://localhost:3000/health | ConvertTo-Json</summary>

```bash
Invoke-RestMethod http://localhost:3000/health | ConvertTo-Json
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\8a43ae99-f9d9-4992-8311-2d9790ab036e.txt</summary>

</details>
</tool-use>

---

Der Server läuft und ist gesund. Aber der alte Server ist ohne meine Änderungen. Ich muss prüfen ob tsx watch die Änderungen aufgenommen hat:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\2.txt</summary>

</details>
</tool-use>

---

Ich sehe, dass die neue Version noch nicht geladen wurde. Ich muss den Benutzer darüber informieren. Lassen Sie mich die Event-Ingestion-Route trotzdem testen, um zu sehen, ob die neue Version bereits aktiv ist:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: $headers = @{"X-API-Key"="test:testsource"; "Content-Type"="application/json"}
$body = @{
    event_type = "page_visited"
    source = "portal"
    occurred_at = "2026-01-21T12:00:00Z"
    lead_identifier = @{
        email = "test@example.com"
    }
    metadata = @{
        page_path = "/pricing"
    }
} | ConvertTo-Json -Depth 5
Invoke-RestMethod -Uri "http://localhost:3000/api/v1/events/ingest" -Method Post -Headers $headers -Body $body | ConvertTo-Json</summary>

```bash
$headers = @{"X-API-Key"="test:testsource"; "Content-Type"="application/json"}
$body = @{
    event_type = "page_visited"
    source = "portal"
    occurred_at = "2026-01-21T12:00:00Z"
    lead_identifier = @{
        email = "test@example.com"
    }
    metadata = @{
        page_path = "/pricing"
    }
} | ConvertTo-Json -Depth 5
Invoke-RestMethod -Uri "http://localhost:3000/api/v1/events/ingest" -Method Post -Headers $headers -Body $body | ConvertTo-Json
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\9be565eb-bedb-40b1-8135-cf68ef0f7cfa.txt</summary>

</details>
</tool-use>

---

Die Route `/api/v1/events/ingest` funktioniert! Wir bekommen einen 401-Fehler (Invalid API Key), was bedeutet, dass die Route registriert ist und die API-Key-Middleware arbeitet. Der Fehler ist, dass ich den falschen API-Key verwendet habe. Lassen Sie mich die `.env` Datei lesen, um die richtigen API-Keys zu finden:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\env.example</summary>

</details>
</tool-use>

---

Jetzt verstehe ich - die API-Keys sind im Format `key:source`. Lassen Sie mich mit dem korrekten Key testen:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: $headers = @{"X-API-Key"="dev_key_portal"; "Content-Type"="application/json"}
$body = @{
    event_type = "page_visited"
    source = "portal"
    occurred_at = "2026-01-21T12:00:00Z"
    lead_identifier = @{
        email = "test@example.com"
    }
    metadata = @{
        page_path = "/pricing"
    }
} | ConvertTo-Json -Depth 5
try {
    $response = Invoke-RestMethod -Uri "http://localhost:3000/api/v1/events/ingest" -Method Post -Headers $headers -Body $body
    $response | ConvertTo-Json
} catch {
    $_.Exception.Response
    $streamReader = [System.IO.StreamReader]::new($_.Exception.Response.GetResponseStream())
    $streamReader.ReadToEnd()
}</summary>

```bash
$headers = @{"X-API-Key"="dev_key_portal"; "Content-Type"="application/json"}
$body = @{
    event_type = "page_visited"
    source = "portal"
    occurred_at = "2026-01-21T12:00:00Z"
    lead_identifier = @{
        email = "test@example.com"
    }
    metadata = @{
        page_path = "/pricing"
    }
} | ConvertTo-Json -Depth 5
try {
    $response = Invoke-RestMethod -Uri "http://localhost:3000/api/v1/events/ingest" -Method Post -Headers $headers -Body $body
    $response | ConvertTo-Json
} catch {
    $_.Exception.Response
    $streamReader = [System.IO.StreamReader]::new($_.Exception.Response.GetResponseStream())
    $streamReader.ReadToEnd()
}
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\cec51811-2940-4690-b7bd-560b43a58e82.txt</summary>

</details>
</tool-use>

---

**Exzellent!** 🎉 Die Event Ingestion funktioniert! Das Event wurde erfolgreich in die Queue gestellt:

```json
{
    "success": true,
    "event_id": "fa58322a-3024-431d-8952-02ca8c3de43c",
    "message": "Event queued for processing",
    "queued_at": "2026-01-21T15:44:52.547Z"
}
```

Jetzt muss ich den Worker starten, um das Event zu verarbeiten. Lassen Sie mich die package.json prüfen, um den workers-Befehl zu sehen:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: E:\dname\crm_dna-me\package.json</summary>

</details>
</tool-use>

---

Ich füge einen `dev:workers` Script hinzu:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: E:\dname\crm_dna-me\package.json</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -10,4 +10,5 @@
      "start": "node dist/index.js",
      "workers": "tsx src/workers/index.ts",
+     "dev:workers": "tsx watch src/workers/index.ts",
      "migrate": "node-pg-migrate",
      "migrate:up": "node-pg-migrate up",
```

</details>

</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: Set-Location E:\dname\crm_dna-me; npm run workers</summary>

```bash
Set-Location E:\dname\crm_dna-me; npm run workers
```
</details>
</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: Start-Sleep -Seconds 3</summary>

```bash
Start-Sleep -Seconds 3
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\7.txt</summary>

</details>
</tool-use>

---

**Perfekt!** 🎉 Der Worker läuft und hat das Event erfolgreich verarbeitet:

1. ✅ Event empfangen: `fa58322a-3024-431d-8952-02ca8c3de43c`
2. ✅ Neuer Lead erstellt: `c44dd55a-aefb-4712-a008-d8b805178df3` (test@example.com)
3. ✅ Event in der Datenbank gespeichert
4. ✅ Job abgeschlossen

Lassen Sie mich einen weiteren Test mit dem Bulk-Import durchführen und dann die Datenbank prüfen:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: $headers = @{"X-API-Key"="dev_key_portal"; "Content-Type"="application/json"}
$body = @{
    leads = @(
        @{
            email = "jane.doe@biotech.com"
            first_name = "Jane"
            last_name = "Doe"
            job_title = "Research Director"
            company_name = "BioTech Labs"
        },
        @{
            email = "john.smith@enterprise.de"
            first_name = "John"
            last_name = "Smith"
            job_title = "Head of Procurement"
            company_name = "Enterprise GmbH"
        }
    )
    source = "import"
    campaign_id = "bulk_test_2026"
    skip_duplicates = $true
} | ConvertTo-Json -Depth 5
try {
    $response = Invoke-RestMethod -Uri "http://localhost:3000/api/v1/leads/bulk" -Method Post -Headers $headers -Body $body
    $response | ConvertTo-Json
} catch {
    $streamReader = [System.IO.StreamReader]::new($_.Exception.Response.GetResponseStream())
    $streamReader.ReadToEnd()
}</summary>

```bash
$headers = @{"X-API-Key"="dev_key_portal"; "Content-Type"="application/json"}
$body = @{
    leads = @(
        @{
            email = "jane.doe@biotech.com"
            first_name = "Jane"
            last_name = "Doe"
            job_title = "Research Director"
            company_name = "BioTech Labs"
        },
        @{
            email = "john.smith@enterprise.de"
            first_name = "John"
            last_name = "Smith"
            job_title = "Head of Procurement"
            company_name = "Enterprise GmbH"
        }
    )
    source = "import"
    campaign_id = "bulk_test_2026"
    skip_duplicates = $true
} | ConvertTo-Json -Depth 5
try {
    $response = Invoke-RestMethod -Uri "http://localhost:3000/api/v1/leads/bulk" -Method Post -Headers $headers -Body $body
    $response | ConvertTo-Json
} catch {
    $streamReader = [System.IO.StreamReader]::new($_.Exception.Response.GetResponseStream())
    $streamReader.ReadToEnd()
}
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\f0cec6fe-7009-4436-894f-ec704c8f3ea8.txt</summary>

</details>
</tool-use>

---

**Hervorragend!** Bulk-Import funktioniert auch:

```json
{
    "success": true,
    "message": "2 leads queued for import",
    "batch_id": "05b51a70-ad2d-4a29-ae6b-4c69c57194b3",
    "total_leads": 2,
    "queued_at": "2026-01-21T15:45:37.991Z"
}
```

Prüfen wir den Worker-Output:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\terminals\7.txt</summary>

</details>
</tool-use>

---

**Ausgezeichnet!** 🎉 Beide Bulk-Import Leads wurden erfolgreich verarbeitet:

1. ✅ Jane Doe (jane.doe@biotech.com) - Research Director bei BioTech Labs
2. ✅ John Smith (john.smith@enterprise.de) - Head of Procurement bei Enterprise GmbH

Jetzt prüfen wir die Datenbank:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: docker exec crm_dna-me-postgres-1 psql -U dna -d dna_marketing -c "SELECT id, email, first_name, last_name, job_title, status, routing_status, first_touch_source FROM leads;"</summary>

```bash
docker exec crm_dna-me-postgres-1 psql -U dna -d dna_marketing -c "SELECT id, email, first_name, last_name, job_title, status, routing_status, first_touch_source FROM leads;"
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\66ef0c60-1563-4319-a2cb-f3f1350bb36c.txt</summary>

</details>
</tool-use>

---

Lassen Sie mich den korrekten Container-Namen finden:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: docker ps --format "{{.Names}}"</summary>

```bash
docker ps --format "{{.Names}}"
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\9ed051ca-6522-49ff-854d-abd3c07c8db3.txt</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: docker exec dna_postgres psql -U dna -d dna_marketing -c "SELECT id, email, first_name, last_name, job_title, status, routing_status, first_touch_source FROM leads;"</summary>

```bash
docker exec dna_postgres psql -U dna -d dna_marketing -c "SELECT id, email, first_name, last_name, job_title, status, routing_status, first_touch_source FROM leads;"
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\e5cde3d1-aaa0-48d4-b578-c714dd62da5d.txt</summary>

</details>
</tool-use>

---

**Perfekt!** 🎉 Alle 3 Leads sind korrekt in der Datenbank:

| Email | Name | Job Title | Status | Routing | Source |
|-------|------|-----------|--------|---------|--------|
| test@example.com | - | - | new | unrouted | portal |
| jane.doe@biotech.com | Jane Doe | Research Director | new | unrouted | import |
| john.smith@enterprise.de | John Smith | Head of Procurement | new | unrouted | import |

Prüfen wir auch die Events:

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: docker exec dna_postgres psql -U dna -d dna_marketing -c "SELECT id, lead_id, event_type, source, processed_at FROM events ORDER BY created_at;"</summary>

```bash
docker exec dna_postgres psql -U dna -d dna_marketing -c "SELECT id, lead_id, event_type, source, processed_at FROM events ORDER BY created_at;"
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\Александр\.cursor\projects\e-dname-crm-dna-me\agent-tools\360519d1-78a3-4dd4-ae7a-9305a7b4252e.txt</summary>

</details>
</tool-use>

---

**Exzellent!** Alle 3 Events wurden gespeichert und verarbeitet:

| Event Type | Source | Processed |
|------------|--------|-----------|
| page_visited | portal | ✅ |
| lead_imported | import | ✅ |
| lead_imported | import | ✅ |

Die Etappe 4: Event Ingestion ist vollständig implementiert. Lassen Sie mich die TODO-Liste aktualisieren und eine Zusammenfassung geben:

---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Etappe 4: Event Ingestion - Middleware, Queues, Events Route, Worker

</details>
</tool-use>

---

## ✅ Etappe 4: Event Ingestion - Vollständig implementiert!

Ich habe alle Komponenten für die Event Ingestion erstellt und erfolgreich getestet:

### Erstellte Dateien:

1. **`src/api/middleware/apiKey.ts`** - API-Key Validierung
   - Validiert `X-API-Key` Header
   - Mappt Key zu Source für Audit-Logging
   - Unterstützt optionale Auth für bestimmte Endpoints

2. **`src/api/middleware/hmac.ts`** - HMAC Signatur-Validierung
   - SHA256-basierte Webhook-Signatur-Prüfung
   - Timing-Safe Vergleich gegen Timing-Attacken
   - Timestamp-Validierung gegen Replay-Attacken
   - Helper-Funktionen für Test-Signatur-Generierung

3. **`src/api/schemas/events.ts`** - Zod Validierungsschemas
   - `leadIdentifierSchema` - Mindestens ein Identifier erforderlich
   - `ingestEventSchema` - Event-Ingestion-Validierung
   - `bulkImportSchema` - Bulk-Lead-Import

4. **`src/api/routes/events.ts`** - API Routen
   - `POST /api/v1/events/ingest` - Event-Aufnahme mit API-Key Auth
   - `POST /api/v1/events/webhook` - Webhook mit HMAC-Validierung
   - `POST /api/v1/leads/bulk` - Massen-Import bis 1000 Leads

5. **`src/workers/eventWorker.ts`** - BullMQ Event Worker
   - Find-or-Create Lead Logik
   - Event-Speicherung mit Partitionierung
   - Attribution-Tracking (First/Last Touch)
   - Concurrency: 10, Rate Limit: 100/Sekunde

### Aktualisierte Dateien:
- `src/index.ts` - Events Routes registriert
- `src/workers/index.ts` - Event Worker integriert
- `package.json` - `dev:workers` Script hinzugefügt

### Test-Ergebnisse:
```
✅ Health Check: Server läuft (healthy)
✅ Event Ingestion: page_visited Event verarbeitet
✅ Bulk Import: 2 Leads importiert
✅ Worker: Alle 3 Jobs erfolgreich abgeschlossen
✅ Datenbank: 3 Leads, 3 Events gespeichert
```

---

